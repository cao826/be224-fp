{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206a30ec",
   "metadata": {},
   "source": [
    "# Running dataset with basic convolutional model\n",
    "\n",
    "If you want to see some basics about the data and the images, refer to the other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0193252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb72c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data_handling as data\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c352c",
   "metadata": {},
   "source": [
    "## Creating dataset\n",
    "\n",
    "The code for this dataset is included in ```data_handling.py```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2617b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "needle_directory = '/Users/carlosolivares/be224-fp/data/NeedleImages/'\n",
    "labels_path = os.path.join(needle_directory, 'Labels.csv')\n",
    "\n",
    "data_transformer = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataset = data.NeedleImageDataset(\n",
    "    path2data=needle_directory,\n",
    "    path2labels=labels_path,\n",
    "    transform = data_transformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6872bb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train datset length: 504\n",
      "validation dataset length: 127\n"
     ]
    }
   ],
   "source": [
    "len_dataset = len(dataset)\n",
    "len_train = int(0.8 * len_dataset)\n",
    "len_val = len_dataset - len_train\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [len_train, len_val])\n",
    "\n",
    "print(\"train datset length:\", len(train_ds))\n",
    "print(\"validation dataset length:\", len(val_ds))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=10, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8605811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "## checking the length of the dataloaders\n",
    "print(len(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54bb2c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 512, 512])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "checking to see that the batches come appropriately shaped for input to model. \n",
    "The model accepts data in (batch, channels, height, width)\n",
    "Our images are grayscale, so we stack the values to get 3 channels like an RGB\n",
    "\"\"\"\n",
    "\n",
    "for xb, yb in val_dl:\n",
    "    print(xb.shape)\n",
    "    print(yb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb5a2f",
   "metadata": {},
   "source": [
    "## Model option 1\n",
    "\n",
    "This one has two output nodes with no output activation and ```torch.nn.CrossEntropyLoss``` as the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "455bd2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "naivenet = model.NaiveNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b63ef40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape #recall this is the multi-node output thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d9c4c",
   "metadata": {},
   "source": [
    "### Training the model for ten epoch to see if it actually learns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e3d8462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n"
     ]
    }
   ],
   "source": [
    "weights = [.3027, .6973]\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum', weight=torch.tensor(weights))\n",
    "optimizer = optim.Adam(naivenet.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fbebc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 23.558\n",
      "[1,    20] loss: 3.048\n",
      "[1,    30] loss: 3.231\n",
      "[1,    40] loss: 3.052\n",
      "[1,    50] loss: 2.894\n",
      "validation loss after epoch 51: 0.22011610091201902\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "[2,    10] loss: 2.395\n",
      "[2,    20] loss: 2.328\n",
      "[2,    30] loss: 2.555\n",
      "[2,    40] loss: 2.308\n",
      "[2,    50] loss: 2.636\n",
      "validation loss after epoch 51: 0.2083043836233184\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[3,    10] loss: 2.275\n",
      "[3,    20] loss: 2.395\n",
      "[3,    30] loss: 2.648\n",
      "[3,    40] loss: 2.579\n",
      "[3,    50] loss: 2.064\n",
      "validation loss after epoch 51: 0.2011332568221205\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "[4,    10] loss: 2.429\n",
      "[4,    20] loss: 2.379\n",
      "[4,    30] loss: 2.329\n",
      "[4,    40] loss: 2.482\n",
      "[4,    50] loss: 2.180\n",
      "validation loss after epoch 51: 0.1975304376421951\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        naivenet.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = naivenet(inputs)\n",
    "        loss = criterion(outputs, labels.to(torch.int64))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
    "            running_loss = 0.0\n",
    "    val_loss = model.get_validation_loss(naivenet, criterion, val_dl)\n",
    "    print('validation loss after epoch {}: {}'.format((i+1), val_loss))\n",
    "    scheduler.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73865b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ok, it seems to be working, so now all we need is to figure out exactly how to get metrics and loss or whatever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eca3836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## investigating how the crossentrupy loss works. From the docs: \n",
    "\n",
    "# Example of target with class indices\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "#output.backward()\n",
    "# Example of target with class probabilities\n",
    "#input = torch.randn(3, 5, requires_grad=True)\n",
    "#target = torch.randn(3, 5).softmax(dim=1)\n",
    "#output = loss(input, target)\n",
    "#output.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de11edc7",
   "metadata": {},
   "source": [
    "### Looking at how the loss function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e57a794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6759, -0.8502,  0.7985, -0.5464,  1.2984],\n",
       "        [-1.1789, -1.6076, -0.6740,  1.3447, -0.6631],\n",
       "        [-1.3812,  0.5992,  2.3664,  0.8805,  0.3675]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0fe91c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5594c415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0512, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "491881a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4508,  0.3201],\n",
       "        [-0.4884,  0.3111],\n",
       "        [-0.4619,  0.3550],\n",
       "        [-0.5006,  0.3499]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d853cbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d9522da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7609, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31369730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb173171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next thing to do is actually get the metrics from model and stuff. \n",
    "# looks like you might need an lr schedulaer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539e563",
   "metadata": {},
   "source": [
    "## Model option 2\n",
    "\n",
    "Same model but with one output node, sigmoid activation, and ```torch.nn.BCELoss```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd375b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_model = model.NaiveNetSigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "333daf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 0., 1., 0., 1., 1., 0., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in val_dl:\n",
    "    out = sigmoid_model(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "064f9838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4877],\n",
       "        [0.4873],\n",
       "        [0.4885],\n",
       "        [0.4867],\n",
       "        [0.4873],\n",
       "        [0.4872],\n",
       "        [0.4884],\n",
       "        [0.4864],\n",
       "        [0.4860],\n",
       "        [0.4881]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2c76634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.702\n",
      "[1,    20] loss: 0.705\n",
      "[1,    30] loss: 0.705\n",
      "[1,    40] loss: 0.703\n",
      "[1,    50] loss: 0.705\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sj/ly263hxx0vz_tvymz1bwjkgm0000gn/T/ipykernel_5305/2257104516.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_validation_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation loss after epoch {}: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/be224-fp/model.py\u001b[0m in \u001b[0;36mget_validation_loss\u001b[0;34m(model, loss_fn, validation_dataloader)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/basic-pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/basic-pytorch/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/basic-pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2517\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m         raise ValueError(\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\n\u001b[0;32m-> 2519\u001b[0;31m                          \"Please ensure they have the same size.\".format(target.size(), input.size()))\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "optimizer = optim.Adam(naivenet.parameters())\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        sigmoid_model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = sigmoid_model(inputs)\n",
    "        #print(outputs.dtype)\n",
    "        #print(labels.view(10,1).dtype)\n",
    "        loss = criterion(outputs, labels.view(-1, 1).to(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
    "            running_loss = 0.0\n",
    "    val_loss = model.get_validation_loss(sigmoid_model, criterion, val_dl)\n",
    "    print('validation loss after epoch {}: {}'.format((i+1), val_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f0d8aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ data.change_to_numeric_binary(label) for label in dataset.example_label_map.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "601e35b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "631"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.example_label_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099d27b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [.3027, .6973]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basic-pytorch]",
   "language": "python",
   "name": "conda-env-basic-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
