{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206a30ec",
   "metadata": {},
   "source": [
    "# Running dataset with basic convolutional model\n",
    "\n",
    "If you want to see some basics about the data and the images, refer to the other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0193252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb72c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data_handling as data\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c352c",
   "metadata": {},
   "source": [
    "## Creating dataset\n",
    "\n",
    "The code for this dataset is included in ```data_handling.py```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2617b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "needle_directory = '/Users/carlosolivares/be224-fp/data/NeedleImages/'\n",
    "labels_path = os.path.join(needle_directory, 'Labels.csv')\n",
    "\n",
    "data_transformer = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataset = data.NeedleImageDataset(\n",
    "    path2data=needle_directory,\n",
    "    path2labels=labels_path,\n",
    "    transform = data_transformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6872bb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train datset length: 504\n",
      "validation dataset length: 127\n"
     ]
    }
   ],
   "source": [
    "len_dataset = len(dataset)\n",
    "len_train = int(0.8 * len_dataset)\n",
    "len_val = len_dataset - len_train\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [len_train, len_val])\n",
    "\n",
    "print(\"train datset length:\", len(train_ds))\n",
    "print(\"validation dataset length:\", len(val_ds))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54bb2c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "checking to see that the batches come appropriately shaped for input to model. \n",
    "The model accepts data in (batch, channels, height, width)\n",
    "Our images are grayscale, so we stack the values to get 3 channels like an RGB\n",
    "\"\"\"\n",
    "\n",
    "for xb, yb in val_dl:\n",
    "    print(xb.shape)\n",
    "    print(yb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb5a2f",
   "metadata": {},
   "source": [
    "## Getting model working\n",
    "\n",
    "ummm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "455bd2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "naivenet = model.NaiveNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "725b1135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after first convolution: torch.Size([32, 6, 510, 510])\n",
      "Shape after first pool: torch.Size([32, 6, 255, 255])\n",
      "Shape after second convolution: torch.Size([32, 12, 253, 253])\n",
      "Shape after second pool: torch.Size([32, 12, 126, 126])\n",
      "Shape after third convolution: torch.Size([32, 24, 124, 124])\n",
      "Shape after third pool: torch.Size([32, 24, 62, 62])\n",
      "Shape after first convolution: torch.Size([32, 48, 60, 60])\n",
      "Shape after fourth pool: torch.Size([32, 48, 30, 30])\n",
      "shape after flattening: torch.Size([32, 43200])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in val_dl:\n",
    "    naivenet(xb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ef40b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basic-pytorch]",
   "language": "python",
   "name": "conda-env-basic-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
